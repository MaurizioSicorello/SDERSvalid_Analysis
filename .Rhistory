#
# nrow(CFAsimOUt[CFAsimOUt$srmr < .08 & CFAsimOUt$rmsea < .06 & CFAsimOUt$cfi > .95 & CFAsimOUt$tli > .95, ])
####################################
# EFA Lavender
# Note: Principal Axis Factoring is not available in Lavaan and, conversely, not all fit measures are available in other efa applications in R
# Therefore, we first fit the efa model with the exact specification of Lavender et al. in Psych:fa. Then, we use these as starting values
# for a Lavaan efa to get fit measures.
fa.parallel(dfSDERS, fm = "pa", fa = "fa", quant = .99)
# 4 factor model
EFAmodel4 <- fa(dfSDERS, nfactor = 4, rotate = "Promax", fm = "pa")
colnames(EFAmodel4$loadings) <- c("Nonacceptance", "Modulate", "Awareness", "Clarity")
print(EFAmodel4)
print(EFAmodel4$loadings,cutoff = 0.4, sort = TRUE)
print(EFAmodel4$loadings,cutoff = 0.3, sort = TRUE)
plot(EFAmodel4)
diagram(EFAmodel4)
# save table of loadings and structure coefficients created with custom function
save_as_docx(
flextable(
fa_loadingTable(EFAmodel4, cut = -1, structure = TRUE)
),
path = here("manuscripts", "SDERSvalid_crossSec", "tables", "loadings.docx")
)
# BETTER function to make apa_table for factor loadings
# datscience::apa_factorLoadings(psych::fa(EFAmodel4, nfactors = 4))
fitMeasures_EFAmodel4 <- c(EFAmodel4$rms,
EFAmodel4$RMSEA["RMSEA"],
((EFAmodel4$null.chisq-EFAmodel4$null.dof)-(EFAmodel4$STATISTIC-EFAmodel4$dof))/(EFAmodel4$null.chisq-EFAmodel4$null.dof), # CFI, Scource: https://gist.github.com/tonosan/cb7581f3459ae7c4217a
EFAmodel4$TLI,
EFAmodel4$STATISTIC,
EFAmodel4$dof,
EFAmodel4$PVAL)
### rotation of Lavender (promax plus pa)
esem_efaPromax <- esem_efa(data=dfSDERS,
nfactors =4,
fm = 'pa',
rotate="promax",
residuals = TRUE)
esem_modelPromax <- esem_syntax(esem_efaPromax)
writeLines(esem_modelPromax)
esem_fitPromax <- cfa(model=esem_modelPromax, data=dfSDERS, std.lv=TRUE, estimator = "ML")
# inspect model (careful: P1 = Nonaccept, P2 = Awareness, P3 = Modulate, P4 = Clarity)
summary(esem_fitPromax, fit.measures = TRUE, standardized = TRUE)
reliabilityEFACustom(EFAmodel4)
reliabilityEFACustom(EFAmodel4, forItems = main_loadings_list$NonAccept)
reliabilityEFACustom(EFAmodel4, forItems = main_loadings_list$Modulate)
reliabilityEFACustom(EFAmodel4, forItems = main_loadings_list$Awareness)
reliabilityEFACustom(EFAmodel4, forItems = main_loadings_list$Clarity)
cor.test(dfSDERS$S.DERS7_BL, dfSDERS$S.DERS14_BL)
constructNames <- c("DERS_Nonacceptance_sum", "DERS_Awareness_sum", "DERS_Clarity_sum", "DERS_Strategies_sum", "DERS_Impulse_sum", "DERS_Goals_sum", "DERS_Total_sum",
"DASS_Total_sum", "NEO_N_sum", "reactivityValence", "AFFECT_PRE_X", "AFFECT_POST_X", "reactivityArousal", "AFFECT_PRE_Y", "AFFECT_POST_Y", "FAH_Total_Sum", "BIS_Total_sum", "FFAF_Describe_sum", "FFAF_Awareness_sum")
corrOut <- as.data.frame(
matrix(nrow=length(constructNames), ncol=length(sumScaleNames),
dimnames = list(constructNames, sumScaleNames))
)
BayesOut <- as.data.frame(
matrix(nrow=length(constructNames), ncol=length(sumScaleNames),
dimnames = list(constructNames, sumScaleNames))
)
for(i in 1:length(constructNames)){
if(constructNames[i] == "FFAF_Describe_sum" | constructNames[i] == "FFAF_Awareness_sum"){
direction <- 2
}else{
direction <- 1
}
for(j in 1:length(sumScaleNames)){
varConstruct <- df[, constructNames[i]]
varSDERS <- df[, sumScaleNames[j]]
# correlation
corrOut[i,j] <- round(cor(varSDERS, varConstruct), 2)
# Bayes factor
BFsingle <- correlationBF(varSDERS, varConstruct, nullInterval = c(0, 1))
BFsingle <- round(extractBF(BFsingle)[direction, "bf"], 1)
BayesOut[i,j] <- ifelse(BFsingle > 100, ">100", BFsingle)
}
}
corrTab <- flextable(cbind(row.names(corrOut), corrOut))
corrTab <- add_footer_lines(corrTab, paste("r_crit for p < .05 (one-sided):", critical.r.onesided(nrow(df), alpha = .05), "\n",
"r_crit for p < .01 (one-sided):", critical.r.onesided(nrow(df), alpha = .01), "\n",
"r_crit for p < .001 (one-sided):", critical.r.onesided(nrow(df), alpha = .001), "\n"))
corrTab
critical.r.onesided <- function(n, alpha = .05) {
df <- n - 2
critical.t <- qt(alpha, df, lower.tail = F)
critical.r <- sqrt( (critical.t^2) / ( (critical.t^2) + df ) )
return(round(critical.r, 2))
}
# Example usage: Critical correlation coefficient at sample size of n = 100
critical.r.onesided(nrow(df), alpha = .05)
critical.r.onesided(nrow(df), alpha = .01)
critical.r.onesided(nrow(df), alpha = .001)
corrTab
critical.r.onesided <- function(n, alpha = .05) {
df <- n - 2
critical.t <- qt(alpha, df, lower.tail = F)
critical.r <- sqrt( (critical.t^2) / ( (critical.t^2) + df ) )
return(round(critical.r, 2))
}
# Example usage: Critical correlation coefficient at sample size of n = 100
critical.r.onesided(nrow(df), alpha = .05)
critical.r.onesided(nrow(df), alpha = .01)
critical.r.onesided(nrow(df), alpha = .001)
corrTab <- flextable(cbind(row.names(corrOut), corrOut))
corrTab <- add_footer_lines(corrTab, paste("r_crit for p < .05 (one-sided):", critical.r.onesided(nrow(df), alpha = .05), "\n",
"r_crit for p < .01 (one-sided):", critical.r.onesided(nrow(df), alpha = .01), "\n",
"r_crit for p < .001 (one-sided):", critical.r.onesided(nrow(df), alpha = .001), "\n"))
save_as_docx(
corrTab
path = here("manuscripts", "SDERSvalid_crossSec", "tables", "constructValidity.docx")
save_as_docx(
corrTab,
path = here("manuscripts", "SDERSvalid_crossSec", "tables", "constructValidity.docx")
)
corrOut
constructNames
df$FAH_Total_Sum
constructNames <- c("DERS_Nonacceptance_sum", "DERS_Awareness_sum", "DERS_Clarity_sum", "DERS_Strategies_sum", "DERS_Impulse_sum", "DERS_Goals_sum", "DERS_Total_sum",
"DASS_Total_sum", "NEO_N_sum", "reactivityValence", "AFFECT_PRE_X", "AFFECT_POST_X", "reactivityArousal", "AFFECT_PRE_Y", "AFFECT_POST_Y", "FAH_Total_sum", "BIS_Total_sum", "FFAF_Describe_sum", "FFAF_Awareness_sum")
corrOut <- as.data.frame(
matrix(nrow=length(constructNames), ncol=length(sumScaleNames),
dimnames = list(constructNames, sumScaleNames))
)
BayesOut <- as.data.frame(
matrix(nrow=length(constructNames), ncol=length(sumScaleNames),
dimnames = list(constructNames, sumScaleNames))
)
for(i in 1:length(constructNames)){
if(constructNames[i] == "FFAF_Describe_sum" | constructNames[i] == "FFAF_Awareness_sum"){
direction <- 2
}else{
direction <- 1
}
for(j in 1:length(sumScaleNames)){
varConstruct <- df[, constructNames[i]]
varSDERS <- df[, sumScaleNames[j]]
# correlation
corrOut[i,j] <- round(cor(varSDERS, varConstruct), 2)
# Bayes factor
BFsingle <- correlationBF(varSDERS, varConstruct, nullInterval = c(0, 1))
BFsingle <- round(extractBF(BFsingle)[direction, "bf"], 1)
BayesOut[i,j] <- ifelse(BFsingle > 100, ">100", BFsingle)
}
}
corrOut
critical.r.onesided <- function(n, alpha = .05) {
df <- n - 2
critical.t <- qt(alpha, df, lower.tail = F)
critical.r <- sqrt( (critical.t^2) / ( (critical.t^2) + df ) )
return(round(critical.r, 2))
}
critical.r.onesided <- function(n, alpha = .05) {
df <- n - 2
critical.t <- qt(alpha, df, lower.tail = F)
critical.r <- sqrt( (critical.t^2) / ( (critical.t^2) + df ) )
return(round(critical.r, 2))
}
# Example usage: Critical correlation coefficient at sample size of n = 100
critical.r.onesided(nrow(df), alpha = .05)
critical.r.onesided(nrow(df), alpha = .01)
critical.r.onesided(nrow(df), alpha = .001)
corrTab <- flextable(cbind(row.names(corrOut), corrOut))
corrTab <- add_footer_lines(corrTab, paste("r_crit for p < .05 (one-sided):", critical.r.onesided(nrow(df), alpha = .05), "\n",
"r_crit for p < .01 (one-sided):", critical.r.onesided(nrow(df), alpha = .01), "\n",
"r_crit for p < .001 (one-sided):", critical.r.onesided(nrow(df), alpha = .001), "\n"))
save_as_docx(
corrTab,
path = here("manuscripts", "SDERSvalid_crossSec", "tables", "constructValidity.docx")
)
hist(df$BIS_Total_sum)
sumScaleNames
names(df)
cor(df[, c(sumScaleNames, "BIS_NonPlanning_sum", "BIS_Motor_sum", "BIS_Attention_sum", "BIS_Total_sum")])
corrplot(cor(df[, c(sumScaleNames, "BIS_NonPlanning_sum", "BIS_Motor_sum", "BIS_Attention_sum", "BIS_Total_sum")]))
ggsave(here("manuscripts", "SDERSvalid_crossSec", "figures", "BIScorrels.svg"), device="svg")
svg(file=here("manuscripts", "SDERSvalid_crossSec","figures", "BIScorrels.svg"))
corrplot(cor(df[, c(sumScaleNames, "BIS_NonPlanning_sum", "BIS_Motor_sum", "BIS_Attention_sum", "BIS_Total_sum")]))
dev.off()
cor.mtest(df[, c(sumScaleNames, "BIS_Attention_sum")])
cor(df[, c(sumScaleNames, "BIS_Attention_sum")])
mean(df$reactivityValence)
cor(df$AFFECT_POST_X, df$S.DERS_Total_sum)
cor(df$AFFECT_PRE_X, df$S.DERS_Total_sum)
cor(df$reactivityValence, df$S.DERS_Total_sum)
cor(df$AFFECT_POST_X, df$NEO_N_mean)
cor(df$AFFECT_PRE_X, df$NEO_N_mean)
cor(df$AFFECT_POST_X, df$S.DERS_Total_sum)
cor(df$AFFECT_PRE_X, df$S.DERS_Total_sum)
cor.test(df$AFFECT_POST_X, df$S.DERS_Total_sum)
cor.test(df$AFFECT_POST_X, df$S.DERS_Total_sum)
cor.test(df$AFFECT_PRE_X, df$S.DERS_Total_sum)
cor.test(df$AFFECT_POST_X, df$S.DERS_Total_sum)
cor.test(df$AFFECT_POST_X, df$NEO_N_mean)
cor.test(df$AFFECT_PRE_X, df$NEO_N_mean)
cor.test(df$AFFECT_POST_X, df$NEO_N_mean)
cor.test(df$reactivityValence, df$NEO_N_mean)
cor(df$AFFECT_PRE_X, df$AFFECT_POST_X )
library("MASS")
matrix(1,0.5, -0.3, 0.5, 1, -0.1, 1)
matrix(c(1,0.5, -0.3, 0.5, 1, -0.1, 1))
matrix(c(1,0.5, -0.3, 0.5, 1, -0.1, 1), nrow=3)
matrix(c(1,0.5, -0.3, 0.5, 1, -0.1, -0.3, -0.1, 1), nrow=3)
simDat <- mvrnorm(2000, c(0,0,0), matrix(c(1,0.5, -0.3, 0.5, 1, -0.1, -0.3, -0.1, 1), nrow=3))
names(simDat) <- c("pre", "post", "X")
simDat
simDat <- as.data.frame(mvrnorm(2000, c(0,0,0), matrix(c(1,0.5, -0.3, 0.5, 1, -0.1, -0.3, -0.1, 1), nrow=3)))
names(simDat) <- c("pre", "post", "X")
simDat
simDat$react <- simDat$pre - simDat$post
cor(simDat$X, simDat$pre)
cor(simDat$X, simDat$post)
cor(simDat$X, simDat$react)
cor(simDat$X, simDat$pre) - cor(simDat$X, simDat$post)
cov(simDat$X, simDat$pre) - cov(simDat$X, simDat$post)
cov(simDat$X, simDat$react)
simDat <- as.data.frame(mvrnorm(2000, c(0,0,0), matrix(c(1,0.5, -0.3, 0.5, 1, -0.5, -0.3, -0.5, 1), nrow=3)))
names(simDat) <- c("pre", "post", "X")
simDat$react <- simDat$pre - simDat$post
cov(simDat$X, simDat$pre) - cov(simDat$X, simDat$post)
cov(simDat$X, simDat$react)
cov(simDat$X, simDat$pre)
cov(simDat$X, simDat$post)
cov(simDat$X, simDat$react)
names(df)
rowSums(df$S.DERS_NonAccept_mean, df$S.DERS_Modulate_mean, df$DERS_Awareness_mean, df$S.DERS_Clarity_mean)
df$S.DERS_Total_mean_unweighted <- rowSums(df[, c("S.DERS_NonAccept_mean", "S.DERS_Modulate_mean", "DERS_Awareness_mean", "S.DERS_Clarity_mean")])
t.test(data=df, S.DERS_Total_mean_unweighted~MENT.HEALTH)
t.test(data=df, S.DERS_Total_mean~MENT.HEALTH)
cor.test(df$S.DERS_Total_mean_unweighted, df$DASS_Total_mean)
_unweighted
cor.test(df$S.DERS_Total_mean, df$DASS_Total_mean)
cor.test(df$S.DERS_Total_mean_unweighted, df$NEO_N_mean)
cor.test(df$S.DERS_Total_mean, df$NEO_N_mean)
########################################################################
# load packages, custom functions and data
# packages
library("renv")
library("here")
library("apaTables")
library("stringr")
library("psych")
library("ggplot2")
library("svglite")
library("GPArotation")
library("lavaan")
library("FCO")
library("dynamic")
library("semPlot")
library("reshape2")
library("esem")
library("semTools")
library("nonnest2")
library("stats")
library("corrplot")
library("xlsx")
library("flextable")
library("BayesFactor")
library("ppcor")
library("bootnet")
library("qgraph")
#install.packages("psychonetrics")
# custom functions
source(here("functions", "reliabilityEFA.R"))
source(here("functions", "reliabilityESEM.R"))
source(here("functions", "fa_loadingTable.R"))
# data
df <- read.csv(here("data", "SDERSvalid_crossSec_data_preprocessed.csv"))
#####################################################################################
############################# PRELIMINARY ANALYSES ##################################
#####################################################################################
########################################################################
# Sample Descriptives
# demographics
round(table(df$SEX)/length(df$SEX),2)
df$HOCHSCHULE <- ifelse(str_detect(df$HOCHSCHULE, regex('heidelberg', ignore_case = T)), "uni heidelberg",
ifelse(str_detect(df$HOCHSCHULE, regex('regensburg', ignore_case = T)), "uni regensburg",
df$HOCHSCHULE))
table(df$HOCHSCHULE)
round(table(df$OCCUPATION)/length(df$OCCUPATION),2) # 2 = vocational school, 3 = University, 4 = employed, 6 = other
round(table(df$VPN_STUNDEN)/length(df$VPN_STUNDEN),2)
describe(df$AGE)
table(df$ACAD.DEGREE) # 3 = Realschule, 4 = Abitur, 5 = Hochschulabschluss, 6 = Promotion
table(df$FAM.STATUS) # 1 = ledig, 2 = partnerschaft, 3 = verheiratet
# mental health
table(df$MENT.HEALTH)
df$MENT.AGE_num <- as.numeric(ifelse(str_detect(df$MENT.AGE, "^[:digit:]+$"), df$MENT.AGE, NA))
hist(df$MENT.AGE_num)
mean(df$MENT.AGE_num, na.rm = TRUE)
sd(df$MENT.AGE_num, na.rm = TRUE)
table(df$MENT.MEDICINE)
table(df$MENT.TREATMENT)
describe(df$questDuration)
########################################################################
# Psychological Measures
apa.cor.table(df[, c("S.DERS_Total_sum", "DERS_Total_sum", "CTQ_Total_sum", "NEO_N_sum", "IERQ_Total_sum", "BIS_Total_sum", "FAH_Total_sum", "FFAF_Describe_sum", "FFAF_Awareness_sum")],
filename = here("manuscripts", "SDERSvalid_crossSec", "tables", "SDERSbaselineMeasuresDescriptives.doc"))
apa.cor.table(df[, c("S.DERS_Total_sum", "S.DERS_NonAccept_sum", "S.DERS_Modulate_sum" , "S.DERS_Awareness_sum", "S.DERS_Clarity_sum")],
filename = here("manuscripts", "SDERSvalid_crossSec", "tables", "SDERSintercorr_Baseline.doc"))
########################################################################
# Manipulation check
df$reactivityValence <- df$AFFECT_PRE_X - df$AFFECT_POST_X # higher values mean larger increase of negative affect
df$reactivityArousal <- df$AFFECT_PRE_Y - df$AFFECT_POST_Y
cor(df$AFFECT_PRE_X, df$AFFECT_POST_X )
describe(df[, c("AFFECT_PRE_X", "AFFECT_POST_X", "AFFECT_PRE_Y", "AFFECT_POST_Y")])
# valence
t.test(df$AFFECT_PRE_X, df$AFFECT_POST_X, paired = TRUE)
mean(df$AFFECT_POST_X - df$AFFECT_PRE_X)/sd(df$AFFECT_POST_X - df$AFFECT_PRE_X)
mean(df$AFFECT_PRE_X)
sd(df$AFFECT_PRE_X)
mean(df$AFFECT_POST_X)
sd(df$AFFECT_POST_X)
# arousal
t.test(df$AFFECT_PRE_Y, df$AFFECT_POST_Y, paired = TRUE)
mean(df$AFFECT_POST_Y - df$AFFECT_PRE_Y)/sd(df$AFFECT_POST_Y - df$AFFECT_PRE_Y)
mean(df$AFFECT_PRE_Y)
sd(df$AFFECT_PRE_Y)
mean(df$AFFECT_POST_Y)
sd(df$AFFECT_POST_Y)
cor(df$AFFECT_POST_X, df$AFFECT_PRE_X)
dfInduct <- rbind(setNames(df[, c("AFFECT_PRE_X", "AFFECT_PRE_Y")], c("Valence", "Arousal")),
setNames(df[, c("AFFECT_POST_X", "AFFECT_POST_Y")], c("Valence", "Arousal")))
dfInduct$prePost <- rep(c("pre", "post"), each = (nrow(dfInduct)/2))
dfInductMean <- data.frame(Valence = c(mean(df$AFFECT_PRE_X), mean(df$AFFECT_POST_X)),
Arousal = c(mean(df$AFFECT_PRE_Y), mean(df$AFFECT_POST_Y)),
prePost = c("pre", "post"))
ggplot(dfInduct, aes(x=Valence, y=Arousal, group=prePost, colour=prePost)) +
geom_jitter() +
geom_point(data=dfInductMean,  mapping=aes(x = Valence, y = Arousal, shape = prePost), colour = "black", size = 3)
ggsave(here("manuscripts", "SDERSvalid_crossSec", "figures", "MoodInduction.svg"), device="svg")
cor.test(df$AFFECT_POST_X, df$S.DERS_Total_sum)
cor.test(df$AFFECT_PRE_X, df$S.DERS_Total_sum)
cor.test(df$reactivityValence, df$S.DERS_Total_sum)
cor.test(df$AFFECT_POST_X, df$NEO_N_mean)
cor.test(df$AFFECT_PRE_X, df$NEO_N_mean)
cor.test(df$reactivityValence, df$NEO_N_mean)
hist(df$AFFECT_POST_X - df$AFFECT_PRE_X)
table(df$AFFECT_POST_X - df$AFFECT_PRE_X)
sum((df$AFFECT_POST_X - df$AFFECT_PRE_X) < 0)
hist(df$AFFECT_POST_Y - df$AFFECT_PRE_Y)
sum((df$AFFECT_POST_Y - df$AFFECT_PRE_Y) < 0)
hist(df$MANIPULATION_CHECK)
mean(df$MANIPULATION_CHECK)
sd(df$MANIPULATION_CHECK)
sum(df$MANIPULATION_CHECK == 1)
sum(df$MANIPULATION_CHECK <= 2)
#####################################################################################
########################## S-DERS VALIDATION ANALYSES ###############################
#####################################################################################
# save item names
SDERSnames <- c('S.DERS1_BL','S.DERS2_BL.r','S.DERS3_BL','S.DERS4_BL','S.DERS5_BL','S.DERS6_BL.r','S.DERS7_BL',
'S.DERS8_BL','S.DERS9_BL','S.DERS10_BL','S.DERS11_BL.r','S.DERS12_BL','S.DERS13_BL','S.DERS14_BL',
'S.DERS15_BL','S.DERS16_BL.r','S.DERS17_BL','S.DERS18_BL','S.DERS19_BL.r','S.DERS20_BL','S.DERS21_BL')
# assignment of Items to Subscales
main_loadings_list <-   list(
NonAccept  = c("S.DERS8_BL", "S.DERS4_BL", "S.DERS1_BL", "S.DERS5_BL", "S.DERS12_BL", "S.DERS20_BL", "S.DERS18_BL"),
Modulate = c("S.DERS13_BL", "S.DERS17_BL", "S.DERS10_BL", "S.DERS3_BL", "S.DERS15_BL", "S.DERS21_BL", "S.DERS9_BL"),
Awareness = c("S.DERS6_BL.r", "S.DERS11_BL.r", "S.DERS2_BL.r", "S.DERS19_BL.r", "S.DERS16_BL.r"),
Clarity = c("S.DERS14_BL", "S.DERS7_BL")
)
sumScaleNames <- c("S.DERS_NonAccept_sum", "S.DERS_Modulate_sum", "S.DERS_Awareness_sum", "S.DERS_Clarity_sum", "S.DERS_Total_sum")
dfSDERS <- as.data.frame(scale(df[, SDERSnames]))
describe(dfSDERS)
############################################################################
# Check distribution
describe(dfSDERS)
par(mfrow = c(3, 3))
for(i in 1:7){hist(dfSDERS[,i])}
par(mfrow = c(3, 3))
for(i in 8:14){hist(dfSDERS[,i])}
par(mfrow = c(3, 3))
for(i in 15:21){hist(dfSDERS[,i])}
par(mfrow = c(1,1))
describe(df[, c("S.DERS_Total_mean" , "S.DERS_NonAccept_mean", "S.DERS_Modulate_mean", "S.DERS_Awareness_mean", "S.DERS_Clarity_mean")])
############################################################################
# Structural Validity
# plot correlations
svg(file=here("manuscripts", "SDERSvalid_crossSec","figures", "corrPlot.svg"))
corrM <- cor(dfSDERS)
rownames(corrM) <- as.numeric(unlist(str_extract_all(names(dfSDERS), "\\d+")))
colnames(corrM) <- as.numeric(unlist(str_extract_all(names(dfSDERS), "\\d+")))
corrplot(corrM, method = "square", order = 'hclust', addrect = 4, tl.col="black", tl.srt = 0)
dev.off()
####################################
# CFA
CFAmodel_simpel <-   'NonAccept  =~ S.DERS8_BL + S.DERS4_BL + S.DERS1_BL + S.DERS5_BL + S.DERS12_BL + S.DERS20_BL + S.DERS18_BL
Modulate =~ S.DERS13_BL + S.DERS17_BL + S.DERS10_BL + S.DERS3_BL + S.DERS15_BL + S.DERS21_BL + S.DERS9_BL
Awareness =~ S.DERS6_BL.r + S.DERS11_BL.r + S.DERS2_BL.r + S.DERS19_BL.r + S.DERS16_BL.r
Clarity =~ S.DERS14_BL + S.DERS7_BL'
### ML estimator
CFAmodel_simpel_fit <- cfa(CFAmodel_simpel, data = dfSDERS, std.lv = TRUE, estimator = "ML")
summary(CFAmodel_simpel_fit, fit.measures = TRUE, standardize = TRUE)
fitMeasures_CFAmodel_simpel <- fitMeasures(CFAmodel_simpel_fit, c("srmr", "rmsea", "cfi", "tli", "AIC", "BIC", "chisq", "df", "pvalue"), output = "matrix")
fitMeasures_CFAmodel_simpel
### MLM estimator, robust for nonnormality
CFAmodel_simpel_fit_robustNonnormal <- cfa(CFAmodel_simpel, data = dfSDERS, std.lv = TRUE, estimator = "MLM")
summary(CFAmodel_simpel_fit_robustNonnormal, fit.measures = TRUE, standardize = TRUE)
# # dynamic cutoffs
# fits.single <- gen_fit(mod1 = CFAmodel_simpel, assume.mvn = FALSE, x = dfSDERS, rep = 500)
# flex_co(fits = fits.single, index = c("CFI", "SRMR", "RMSEA", "TLI")) # normal: 0.93555044 0.05786768 0.03762022 0.92604149, nonnormal: 0.54147437  0.06175211  0.02060497 -5.61844858
# recommend(fits.single)
# # Alternative to this procedure
# dynamic::cfaHB(CFAmodel_simpel_fit, plot = TRUE)
# simulate performance of Hu & Bentler cutoffs for CFA model
# iterations <- 1000
# CFAsimOUt <- as.data.frame(matrix(nrow=iterations, ncol=4, dimnames=list(NULL, c("srmr", "rmsea", "cfi", "tli"))))
# CFAimpliedCorMat <- inspect(CFAmodel_simpel_fit, what="cor.ov")
# N = nrow(df)
# set.seed(1000)
# for(i in 1:iterations){
#
#   CFAsimDat <- MASS::mvrnorm(n = N, mu = rep(0, nrow(CFAimpliedCorMat)), CFAimpliedCorMat)
#   CFAsimFit <- cfa(CFAmodel_simpel, data = CFAsimDat, std.lv = TRUE, estimator = "ML")
#   CFAsimOUt[i, ] <- fitMeasures(CFAsimFit, c("srmr", "rmsea", "cfi", "tli"))
#
# }
# hist(CFAsimOUt[, "srmr"])
# hist(CFAsimOUt[, "rmsea"])
# hist(CFAsimOUt[, "cfi"])
# hist(CFAsimOUt[, "tli"])
#
# nrow(CFAsimOUt[CFAsimOUt$srmr < .08 & CFAsimOUt$rmsea < .06 & CFAsimOUt$cfi > .95 & CFAsimOUt$tli > .95, ])
####################################
# EFA Lavender
# Note: Principal Axis Factoring is not available in Lavaan and, conversely, not all fit measures are available in other efa applications in R
# Therefore, we first fit the efa model with the exact specification of Lavender et al. in Psych:fa. Then, we use these as starting values
# for a Lavaan efa to get fit measures.
fa.parallel(dfSDERS, fm = "pa", fa = "fa", quant = .99)
# 4 factor model
EFAmodel4 <- fa(dfSDERS, nfactor = 4, rotate = "Promax", fm = "pa")
colnames(EFAmodel4$loadings) <- c("Nonacceptance", "Modulate", "Awareness", "Clarity")
print(EFAmodel4)
print(EFAmodel4$loadings,cutoff = 0.4, sort = TRUE)
print(EFAmodel4$loadings,cutoff = 0.3, sort = TRUE)
plot(EFAmodel4)
diagram(EFAmodel4)
# save table of loadings and structure coefficients created with custom function
save_as_docx(
flextable(
fa_loadingTable(EFAmodel4, cut = -1, structure = TRUE)
),
path = here("manuscripts", "SDERSvalid_crossSec", "tables", "loadings.docx")
)
# Correlations with other constructs
constructNames <- c("DERS_Nonacceptance_sum", "DERS_Awareness_sum", "DERS_Clarity_sum", "DERS_Strategies_sum", "DERS_Impulse_sum", "DERS_Goals_sum", "DERS_Total_sum",
"DASS_Total_sum", "NEO_N_sum", "reactivityValence", "AFFECT_PRE_X", "AFFECT_POST_X", "reactivityArousal", "AFFECT_PRE_Y", "AFFECT_POST_Y", "FAH_Total_sum", "BIS_Total_sum", "FFAF_Describe_sum", "FFAF_Awareness_sum")
corrOut <- as.data.frame(
matrix(nrow=length(constructNames), ncol=length(sumScaleNames),
dimnames = list(constructNames, sumScaleNames))
)
BayesOut <- as.data.frame(
matrix(nrow=length(constructNames), ncol=length(sumScaleNames),
dimnames = list(constructNames, sumScaleNames))
)
for(i in 1:length(constructNames)){
if(constructNames[i] == "FFAF_Describe_sum" | constructNames[i] == "FFAF_Awareness_sum"){
direction <- 2
}else{
direction <- 1
}
for(j in 1:length(sumScaleNames)){
varConstruct <- df[, constructNames[i]]
varSDERS <- df[, sumScaleNames[j]]
# correlation
corrOut[i,j] <- round(cor(varSDERS, varConstruct), 2)
# Bayes factor
BFsingle <- correlationBF(varSDERS, varConstruct, nullInterval = c(0, 1))
BFsingle <- round(extractBF(BFsingle)[direction, "bf"], 1)
BayesOut[i,j] <- ifelse(BFsingle > 100, ">100", BFsingle)
}
}
critical.r.onesided <- function(n, alpha = .05) {
df <- n - 2
critical.t <- qt(alpha, df, lower.tail = F)
critical.r <- sqrt( (critical.t^2) / ( (critical.t^2) + df ) )
return(round(critical.r, 2))
}
# Example usage: Critical correlation coefficient at sample size of n = 100
critical.r.onesided(nrow(df), alpha = .05)
critical.r.onesided(nrow(df), alpha = .01)
critical.r.onesided(nrow(df), alpha = .001)
corrTab <- flextable(cbind(row.names(corrOut), corrOut))
corrTab <- add_footer_lines(corrTab, paste("r_crit for p < .05 (one-sided):", critical.r.onesided(nrow(df), alpha = .05), "\n",
"r_crit for p < .01 (one-sided):", critical.r.onesided(nrow(df), alpha = .01), "\n",
"r_crit for p < .001 (one-sided):", critical.r.onesided(nrow(df), alpha = .001), "\n"))
save_as_docx(
corrTab,
path = here("manuscripts", "SDERSvalid_crossSec", "tables", "constructValidity.docx")
)
corrplot(cor(df[, c(sumScaleNames, "reactivityValence")]))
?centralityPlot
centralityPlot(SDERSnetwork$graph)
SDERSnetwork <- estimateNetwork(dfSDERS, default="EBICglasso", labels=c(1:21))
print(SDERSnetwork)
plot(SDERSnetwork, groups = as.factor(group_assignment))
# Define colors for the four groups
colors <- c("red", "blue", "green", "purple")
# Create a named vector for group assignments
group_assignment <- rep(NA, length(colnames(dfSDERS)))  # Initialize vector with NA
names(group_assignment) <- colnames(dfSDERS)  # Set the names to match your variable names
# Assign each variable to a group
group_assignment[main_loadings_list$NonAccept] <- "Non-Acceptance"
group_assignment[main_loadings_list$Modulate] <- "Modulate"
group_assignment[main_loadings_list$Awareness] <- "Awareness"
group_assignment[main_loadings_list$Clarity] <- "Clarity"
# Create a color vector based on group assignment
nodeColors <- colors[group_assignment]
# Plot the network with custom node colors
qgraph(SDERSnetwork$graph, layout = "spring", labels = c(1:21), groups = as.factor(group_assignment), palette="ggplot2")
centralityPlot(SDERSnetwork$graph)
centralityPlot(SDERSnetwork$graph, scale = "raw0", include = c("Strength",
"Closeness", "Betweenness", "ExpectedInfluence"))
?nonnest2::vuongtest
sessionInfo()
